{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0de9438c2edd40999f346cb5fd1c5442": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DropdownModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DropdownModel",
            "_options_labels": [
              "o4-mini",
              "o3",
              "o3-mini",
              "o1",
              "o1-mini",
              "gpt-4o",
              "gpt-4o-mini",
              "gpt-4.1",
              "gpt-4.1-mini",
              "gpt-4.1-nano",
              "gpt-4",
              "gpt-4-turbo",
              "gpt-3.5-turbo"
            ],
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "DropdownView",
            "description": "Model:",
            "description_tooltip": null,
            "disabled": false,
            "index": 0,
            "layout": "IPY_MODEL_01764b57ffd04692808e765c9d9e51ab",
            "style": "IPY_MODEL_7eec0e352c824ad6983c3763b2a8f874"
          }
        },
        "01764b57ffd04692808e765c9d9e51ab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7eec0e352c824ad6983c3763b2a8f874": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0c2e8072ca66495597fa321c396b7554": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "TextareaModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "TextareaModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "TextareaView",
            "continuous_update": true,
            "description": "Context:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_24cf4ab038494e7ba2e9170a49250136",
            "placeholder": "...",
            "rows": null,
            "style": "IPY_MODEL_0823df1e2047425aa90bb1840283ecdc",
            "value": ""
          }
        },
        "24cf4ab038494e7ba2e9170a49250136": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": "100px",
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "0823df1e2047425aa90bb1840283ecdc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b0451274779c470dacf4f26b22046883": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "TextareaModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "TextareaModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "TextareaView",
            "continuous_update": true,
            "description": "Prompt:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_8a9a42f5596d44cea6012bde6a0d51dc",
            "placeholder": "...",
            "rows": null,
            "style": "IPY_MODEL_7e8c49223b1a422a862bddd417f2d67c",
            "value": ""
          }
        },
        "8a9a42f5596d44cea6012bde6a0d51dc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": "100px",
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "7e8c49223b1a422a862bddd417f2d67c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#  OpenAI API Chat Completions"
      ],
      "metadata": {
        "id": "zlJz8ICnbe4v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Dependencies\n",
        "!pip install --quiet tiktoken\n",
        "!pip install --quiet openai\n",
        "!pip install --quiet pandas\n",
        "!pip install --quiet ipywidgets\n",
        "!pip install --quiet python-dotenv"
      ],
      "metadata": {
        "id": "SvN_sCV4qIrK",
        "cellView": "form"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title OPENAI_API_KEY Environment Variable\n",
        "\n",
        "import dotenv\n",
        "dotenv.load_dotenv('./.env')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "RybtFkx53m0s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Links"
      ],
      "metadata": {
        "id": "9p3TSlNWbuBI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 🧠 Model & Pricing Information\n",
        "- [📄 OpenAI Model Overview](https://platform.openai.com/docs/models) — Learn about GPT models and their capabilities.\n",
        "- [💰 OpenAI Pricing Page](https://openai.com/pricing) — Official token pricing for all models.\n",
        "- [📊 OpenAI Tokenizer Tool](https://platform.openai.com/tokenizer) — Check how your text maps to tokens.\n",
        "\n",
        "#### 🛠️ API & Dashboard\n",
        "- [🧪 OpenAI API Chat Completions Reference](https://platform.openai.com/docs/api-reference/chat) — Official docs for completions, chat, and more.\n",
        "- [📂 OpenAI API Dashboard](https://platform.openai.com/account/usage) — Monitor your token usage and cost.\n",
        "- [🔐 API Key Management](https://platform.openai.com/account/api-keys) — Create and manage your API tokens.\n",
        "\n",
        "#### 📚 Tools & Guides\n",
        "- [🧠 OpenAI Cookbook (GitHub)](https://github.com/openai/openai-cookbook) — Code examples and best practices.\n",
        "- [🧰 OpenAI `tiktoken` Library](https://github.com/openai/tiktoken) — Tokenizer library used by the models.\n",
        "- [✍️ Prompt Engineering Guide](https://github.com/dair-ai/Prompt-Engineering-Guide) — Comprehensive tips and examples.\n",
        "\n"
      ],
      "metadata": {
        "id": "uTn6xOUubkXW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Models"
      ],
      "metadata": {
        "id": "dOt_EmZUbpxC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> 🛠️ Initialization of openai, models with their costs"
      ],
      "metadata": {
        "id": "S-TMmB_6bxS5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Initalization\n",
        "models_list = {\n",
        "    \"o4-mini\":            {\"input\": 1.10,\t\"cached_input\": 0.275,\t\"output\": 0.20},\n",
        "    \"o3\":                 {\"input\": 10.00, \t\"cached_input\": 2.50, \t\"output\": 40.00},\n",
        "    \"o3-mini\":            {\"input\": 1.10, \t\"cached_input\": 0.55, \t\"output\": 4.40},\n",
        "    \"o1\":                 {\"input\": 15.00, \t\"cached_input\": 7.50, \t\"output\": 60.00},\n",
        "    \"o1-mini\":            {\"input\": 1.10, \t\"cached_input\": 0.55, \t\"output\": 4.40},\n",
        "    \"gpt-4o\":             {\"input\": 2.50, \t\"cached_input\": 1.25, \t\"output\": 10.00},\n",
        "    \"gpt-4o-mini\":        {\"input\": 0.15, \t\"cached_input\": 0.075, \t\"output\": 0.60},\n",
        "    \"gpt-4.1\":            {\"input\": 2.00, \t\"cached_input\": 0.50, \t\"output\": 8.00},\n",
        "    \"gpt-4.1-mini\":       {\"input\": 0.40, \t\"cached_input\": 0.10, \t\"output\": 1.60},\n",
        "    \"gpt-4.1-nano\":       {\"input\": 0.10, \t\"cached_input\": 0.025, \t\"output\": 0.40},\n",
        "    \"gpt-4\":              {\"input\": 30.00, \t\"cached_input\": None, \t\"output\": 60.00},\n",
        "    \"gpt-4-turbo\":        {\"input\": 10.00, \t\"cached_input\": None, \t\"output\": 30.00},\n",
        "    \"gpt-3.5-turbo\":      {\"input\": 0.50, \t\"cached_input\": None, \t\"output\": 1.50},\n",
        "}\n",
        "\n",
        "available_models = list(models_list.keys())"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Ej6suaLfb0F9"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 🧠 Models: Use Cases"
      ],
      "metadata": {
        "id": "PqtiJQZ6cAJE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Model             | Good Use Cases                                              | Not Ideal For                                              | Price Sensitive |\n",
        "|------------------|-------------------------------------------------------------|-------------------------------------------------------------|-----------------|\n",
        "| **o4-mini**       | • Balanced reasoning + cost <br>• Coding and math helpers   | • Ultra-fast tasks <br>• High-fidelity generation           | ✅✅             |\n",
        "| **o3**            | • Advanced math and code <br>• Expert tutoring agents       | • Cheap deployment <br>• Simple text processing             | ❌❌❌           |\n",
        "| **o3-mini**       | • Customer support bots <br>• Basic analysis                | • Complex logic <br>• Audio/image understanding             | ✅              |\n",
        "| **o1**            | • Formal logic chains <br>• Strategic AI planning           | • Real-time UIs <br>• Cost-sensitive scaling                | ❌❌             |\n",
        "| **o1-mini**       | • Email summarization <br>• Medium-depth assistants         | • Deep reasoning <br>• Creative writing                     | ✅              |\n",
        "| **gpt-4o**        | • Multimodal chat (vision/audio) <br>• Assistants at scale  | • On-device use <br>• Bulk simple queries                   | ✅              |\n",
        "| **gpt-4o-mini**   | • Chatbots at scale <br>• Fast classification tasks         | • Deep summarization <br>• Complex context                  | ✅✅✅           |\n",
        "| **gpt-4.1**       | • Complex reasoning <br>• Document understanding            | • Real-time apps <br>• Budget tasks                         | ❌              |\n",
        "| **gpt-4.1-mini**  | • General assistants <br>• Customer chat                    | • Expert logic <br>• Fast games                             | ✅✅             |\n",
        "| **gpt-4.1-nano**  | • Input validation <br>• Edge/fast inference                | • Rich generation <br>• Nuanced conversation                | ✅✅✅           |\n",
        "| **gpt-4**         | • Legal, medical, research <br>• Best-in-class generation   | • Anything with cost limits <br>• High-volume chat          | ❌❌❌           |\n",
        "| **gpt-4-turbo**   | • Premium AI copilots <br>• Multiturn apps                  | • Mobile agents <br>• Cheap API endpoints                   | ❌              |\n",
        "| **gpt-3.5-turbo** | • FAQ bots <br>• Affordable AI integration                  | • Creative content <br>• Technical reasoning                | ✅✅             |\n"
      ],
      "metadata": {
        "id": "S8Ukzsu-cFBR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Price List\n",
        "# Correcting the price table rendering by avoiding any pipe characters within the cell values\n",
        "from IPython.display import Markdown, Code, display\n",
        "\n",
        "# Calculate per-token pricing\n",
        "def compute_price_table(prices):\n",
        "    rows = []\n",
        "    for model, price in prices.items():\n",
        "        input_per_token = price[\"input\"] / 1_000_000\n",
        "        output_per_token = price[\"output\"] / 1_000_000\n",
        "        cached_input_token = (\n",
        "            f\"\\${price['cached_input'] / 1_000_000:.6f}\" if price[\"cached_input\"] is not None else \"N/A\"\n",
        "        )\n",
        "        rows.append({\n",
        "            \"model\": model,\n",
        "            \"input_1M\": f\"\\${price['input']:.2f}\",\n",
        "            \"cached_input_1M\": f\"\\${price['cached_input']:.2f}\" if price[\"cached_input\"] is not None else \"N/A\",\n",
        "            \"output_1M\": f\"\\${price['output']:.2f}\",\n",
        "            \"input_token\": f\"\\${input_per_token:.6f}\",\n",
        "            \"cached_input_token\": cached_input_token,\n",
        "            \"output_token\": f\"\\${output_per_token:.6f}\",\n",
        "        })\n",
        "    return rows\n",
        "\n",
        "# Convert to Markdown\n",
        "def to_markdown_table(rows):\n",
        "    headers = [\"Model\", \"Input (1M)\", \"Cached Input (1M)\", \"Output (1M)\",\n",
        "               \"Input/token\", \"Cached Input/token\", \"Output/token\"]\n",
        "    header_line = \"| \" + \" | \".join(headers) + \" |\"\n",
        "    separator = \"| \" + \" | \".join([\"---\"] * len(headers)) + \" |\"\n",
        "    lines = [header_line, separator]\n",
        "    for row in rows:\n",
        "        line = f\"| {row['model']} | {row['input_1M']} | {row['cached_input_1M']} | {row['output_1M']} | \" \\\n",
        "               f\"{row['input_token']} | {row['cached_input_token']} | {row['output_token']} |\"\n",
        "        lines.append(line)\n",
        "    return \"\\r\\n\".join(lines)\n",
        "\n",
        "# Generate and display table\n",
        "table_data = compute_price_table(models_list)\n",
        "markdown_table = to_markdown_table(table_data)\n",
        "display(Markdown(markdown_table))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "cellView": "form",
        "id": "d3zC0r86cNl9",
        "outputId": "8b72b818-4f1a-421b-801e-b6e8c66bfe7a"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "| Model | Input (1M) | Cached Input (1M) | Output (1M) | Input/token | Cached Input/token | Output/token |\r\n| --- | --- | --- | --- | --- | --- | --- |\r\n| o4-mini | \\$1.10 | \\$0.28 | \\$0.20 | \\$0.000001 | \\$0.000000 | \\$0.000000 |\r\n| o3 | \\$10.00 | \\$2.50 | \\$40.00 | \\$0.000010 | \\$0.000003 | \\$0.000040 |\r\n| o3-mini | \\$1.10 | \\$0.55 | \\$4.40 | \\$0.000001 | \\$0.000001 | \\$0.000004 |\r\n| o1 | \\$15.00 | \\$7.50 | \\$60.00 | \\$0.000015 | \\$0.000008 | \\$0.000060 |\r\n| o1-mini | \\$1.10 | \\$0.55 | \\$4.40 | \\$0.000001 | \\$0.000001 | \\$0.000004 |\r\n| gpt-4o | \\$2.50 | \\$1.25 | \\$10.00 | \\$0.000003 | \\$0.000001 | \\$0.000010 |\r\n| gpt-4o-mini | \\$0.15 | \\$0.07 | \\$0.60 | \\$0.000000 | \\$0.000000 | \\$0.000001 |\r\n| gpt-4.1 | \\$2.00 | \\$0.50 | \\$8.00 | \\$0.000002 | \\$0.000000 | \\$0.000008 |\r\n| gpt-4.1-mini | \\$0.40 | \\$0.10 | \\$1.60 | \\$0.000000 | \\$0.000000 | \\$0.000002 |\r\n| gpt-4.1-nano | \\$0.10 | \\$0.03 | \\$0.40 | \\$0.000000 | \\$0.000000 | \\$0.000000 |\r\n| gpt-4 | \\$30.00 | N/A | \\$60.00 | \\$0.000030 | N/A | \\$0.000060 |\r\n| gpt-4-turbo | \\$10.00 | N/A | \\$30.00 | \\$0.000010 | N/A | \\$0.000030 |\r\n| gpt-3.5-turbo | \\$0.50 | N/A | \\$1.50 | \\$0.000000 | N/A | \\$0.000002 |"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 💬 Input"
      ],
      "metadata": {
        "id": "k5Ag48L3vCKk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 🔘 Model Selection\n",
        "# @markdown Use this dropdown to select which model you want to simulate or test within the notebook. This model will be used throughout the context and prompt sections below for cost estimation and token usage tracking.\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "\n",
        "modelSelection = widgets.Dropdown(\n",
        "    options=available_models,\n",
        "    value=available_models[0],\n",
        "    description='Model:',\n",
        "    disabled=False,\n",
        ")\n",
        "\n",
        "display(modelSelection)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "0de9438c2edd40999f346cb5fd1c5442",
            "01764b57ffd04692808e765c9d9e51ab",
            "7eec0e352c824ad6983c3763b2a8f874"
          ]
        },
        "cellView": "form",
        "id": "hJqU7DeurIJ5",
        "outputId": "41bebe07-3330-488b-f557-facac39b5f35"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Dropdown(description='Model:', options=('o4-mini', 'o3', 'o3-mini', 'o1', 'o1-mini', 'gpt-4o', 'gpt-4o-mini', …"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0de9438c2edd40999f346cb5fd1c5442"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 🧾 Model Context\n",
        "# @markdown The context section allows you to simulate or input a system prompt, previous conversation history, or predefined settings that might be passed with each request. Context tokens contribute to input cost and are useful for estimating token budgets ahead of time.\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "\n",
        "# Create two textarea widgets\n",
        "contextInput = widgets.Textarea(\n",
        "    value='',\n",
        "    placeholder='...',\n",
        "    description='Context:',\n",
        "    layout=widgets.Layout(width='100%', height='100px')\n",
        ")\n",
        "\n",
        "display(contextInput)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141,
          "referenced_widgets": [
            "0c2e8072ca66495597fa321c396b7554",
            "24cf4ab038494e7ba2e9170a49250136",
            "0823df1e2047425aa90bb1840283ecdc"
          ]
        },
        "cellView": "form",
        "id": "fbam5XVpoEPF",
        "outputId": "7f28dd62-593d-4d8e-8531-a2e7a0b2a400"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Textarea(value='', description='Context:', layout=Layout(height='100px', width='100%'), placeholder='...')"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0c2e8072ca66495597fa321c396b7554"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title -\n",
        "# @markdown 📏 Precalculate Context Token Count\n",
        "import tiktoken\n",
        "\n",
        "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
        "context_token_count = len(enc.encode(contextInput.value))\n",
        "\n",
        "display(Markdown(f\"Context Token Count: {context_token_count}\"))"
      ],
      "metadata": {
        "cellView": "form",
        "id": "R7yoHM7SpSff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 💬 Prompt\n",
        "# @markdown Here you can input the actual user message or prompt that the model should respond to. This is what would typically be typed in a chat interface or passed via API.\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "\n",
        "# Create two textarea widgets\n",
        "promptInput = widgets.Textarea(\n",
        "    value='',\n",
        "    placeholder='...',\n",
        "    description='Prompt:',\n",
        "    layout=widgets.Layout(width='100%', height='100px')\n",
        ")\n",
        "\n",
        "display(promptInput)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141,
          "referenced_widgets": [
            "b0451274779c470dacf4f26b22046883",
            "8a9a42f5596d44cea6012bde6a0d51dc",
            "7e8c49223b1a422a862bddd417f2d67c"
          ]
        },
        "cellView": "form",
        "id": "ni14NBKgsuKn",
        "outputId": "dc53723c-9a1f-458b-f568-a8ec5c9bc412"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Textarea(value='', description='Prompt:', layout=Layout(height='100px', width='100%'), placeholder='...')"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b0451274779c470dacf4f26b22046883"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title -\n",
        "# @markdown 📏 Precalculate Prompt Token Count\n",
        "import tiktoken\n",
        "\n",
        "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
        "context_token_count = len(enc.encode(promptInput.value))\n",
        "\n",
        "display(Markdown(f\"Context Token Count: {context_token_count}\"))"
      ],
      "metadata": {
        "cellView": "form",
        "id": "LQvMuovHumx4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 💸 Input Costs\n",
        "# @markdown This section calculates the estimated cost of uncached input tokens — including both the system context and user prompt that are sent to the model. It uses the selected model’s pricing to determine the cost of tokens that require fresh processing (not served from cache). Cached inputs are not included here. This helps you understand the baseline cost of new inputs in each interaction.\n",
        "\n",
        "import tiktoken\n",
        "\n",
        "# Get token counts\n",
        "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
        "context_tokens = len(enc.encode(contextInput.value))\n",
        "prompt_tokens = len(enc.encode(promptInput.value))\n",
        "total_input_tokens = context_tokens + prompt_tokens\n",
        "\n",
        "# Calculate cost based on selected model\n",
        "input_cost_per_token = models_list[modelSelection.value][\"input\"] / 1_000_000  # Convert from per million to per token\n",
        "total_input_cost = total_input_tokens * input_cost_per_token\n",
        "\n",
        "# Display results\n",
        "display(Markdown(f\"\"\"\n",
        "### Input Token Counts & Costs\n",
        "- Context tokens: {context_tokens:,}\n",
        "- Prompt tokens: {prompt_tokens:,}\n",
        "- Total input tokens: {total_input_tokens:,}\n",
        "- Cost per token: \\${input_cost_per_token:.6f}\n",
        "- **Total input cost: \\${total_input_cost:.4f}**\n",
        "\"\"\"))"
      ],
      "metadata": {
        "cellView": "form",
        "id": "4_8HjNKhvpMI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "ULo9lOLdsdKi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ⚠️ Warning: Cost-Generating Section\n",
        "\n",
        "⚠️ Note: Executing the API call in the following section may result in usage charges based on the selected model and token count. Make sure to review the context and prompt content beforehand and be aware of the token pricing. Use the token counters above to estimate costs before triggering execution, especially when working with expensive models like GPT-4.1 or OpenAI o3."
      ],
      "metadata": {
        "id": "4kO3GdGhsTdc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "xJ7ufQwSsfq8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 📥 API Response\n",
        "# @markdown This section displays the actual response returned by the selected OpenAI model when given the context and prompt. It helps you verify whether the model is producing the expected output, both in content and format. In real-world scenarios, this is the main data you'd capture and display to users or pipe into downstream logic (e.g., chat apps, data extractors, summarizers). Reviewing the output here is crucial for validating prompt quality, assessing model suitability, and tuning for better performance or cost.\n",
        "\n",
        "from openai import OpenAI\n",
        "import pandas as pd\n",
        "from IPython.display import display, Code, Markdown\n",
        "\n",
        "model = modelSelection.value\n",
        "context = contextInput.value\n",
        "prompt = promptInput.value\n",
        "\n",
        "client = OpenAI()\n",
        "\n",
        "completion = client.chat.completions.create(\n",
        "    model=model,\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"developer\",\n",
        "            \"content\": context\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": prompt\n",
        "        }\n",
        "\n",
        "    ]\n",
        ")\n",
        "\n",
        "result_content = completion.choices[0].message.content\n",
        "usage = completion.usage\n",
        "\n",
        "display(Markdown(result_content))"
      ],
      "metadata": {
        "id": "9VmOmZgewcKu",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 📊 Used Tokens\n",
        "# @markdown This section visualizes token usage, split by type: prompt, completion, cached, etc. It provides a real-time look into how many tokens each part of the interaction consumes, which is key to understanding model behavior and estimating costs.\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "prompt_tokens = usage.prompt_tokens\n",
        "completion_tokens = usage.completion_tokens\n",
        "total_tokens = usage.total_tokens\n",
        "\n",
        "# Extract detailed token usage\n",
        "completion_details = usage.completion_tokens_details\n",
        "prompt_details = usage.prompt_tokens_details\n",
        "\n",
        "accepted_prediction_tokens = completion_details.accepted_prediction_tokens\n",
        "rejected_prediction_tokens = completion_details.rejected_prediction_tokens\n",
        "reasoning_tokens = completion_details.reasoning_tokens\n",
        "completion_audio_tokens = completion_details.audio_tokens\n",
        "\n",
        "cached_prompt_tokens = prompt_details.cached_tokens\n",
        "prompt_audio_tokens = prompt_details.audio_tokens\n",
        "\n",
        "token_data = {\n",
        "    \"Prompt Tokens\": prompt_tokens,\n",
        "    \"Completion Tokens\": completion_tokens,\n",
        "    \"Total Tokens\": total_tokens,\n",
        "    \"Accepted Prediction Tokens\": accepted_prediction_tokens,\n",
        "    \"Rejected Prediction Tokens\": rejected_prediction_tokens,\n",
        "    \"Reasoning Tokens\": reasoning_tokens,\n",
        "    \"Completion Audio Tokens\": completion_audio_tokens,\n",
        "    \"Prompt Audio Tokens\": prompt_audio_tokens,\n",
        "    \"Cached Prompt Tokens\": cached_prompt_tokens\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(token_data.items(), columns=[\"Token Type\", \"Count\"])\n",
        "display(df)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "NvT02Yge0eQt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 💵 Costs\n",
        "# @markdown Based on token usage and the selected model's pricing, this section shows the total cost of the interaction. It breaks down the cost of uncached input, cached input, and output tokens so you can pinpoint where costs are coming from.\n",
        "\n",
        "def get_model_token_prices(model_name: str, models_list: dict):\n",
        "    \"\"\"\n",
        "    Returns input, cached input, and output prices per token for a given model using models_list pricing.\n",
        "\n",
        "    Usage:\n",
        "        get_model_token_prices(\"o4-mini\", models_list)\n",
        "\n",
        "    :param model_name: Name of the model (case-insensitive, flexible format)\n",
        "    :param models_list: Dictionary containing model pricing information\n",
        "    :return: dict with per-token prices or None if not found\n",
        "    \"\"\"\n",
        "    if model_name not in models_list:\n",
        "        print(f\"Model '{model_name}' not found.\")\n",
        "        return None\n",
        "\n",
        "    million = 1_000_000\n",
        "    model_prices = models_list[model_name]\n",
        "\n",
        "    # Handle case where cached_input might be None\n",
        "    cached_input_price = model_prices[\"cached_input\"] if model_prices[\"cached_input\"] is not None else 0.0\n",
        "\n",
        "    per_token_prices = {\n",
        "        \"input_per_token\": model_prices[\"input\"] / million,\n",
        "        \"cached_input_per_token\": cached_input_price / million,\n",
        "        \"output_per_token\": model_prices[\"output\"] / million\n",
        "    }\n",
        "    return per_token_prices\n",
        "\n",
        "# Get prices for selected model\n",
        "prices = get_model_token_prices(model, models_list)\n",
        "\n",
        "if prices is None:\n",
        "    print(\"Could not calculate costs - model pricing not found\")\n",
        "else:\n",
        "    model_input_cost = prices['input_per_token']\n",
        "    model_cached_input_cost = prices['cached_input_per_token']\n",
        "    model_output_cost = prices['output_per_token']\n",
        "\n",
        "    # Calculate costs\n",
        "    # For uncached tokens\n",
        "    uncached_tokens = prompt_tokens - cached_prompt_tokens\n",
        "    uncached_input_cost = uncached_tokens * model_input_cost\n",
        "\n",
        "    # For cached tokens\n",
        "    cached_input_cost = cached_prompt_tokens * model_cached_input_cost\n",
        "\n",
        "    # Output tokens cost\n",
        "    output_cost = completion_tokens * model_output_cost\n",
        "\n",
        "    # Total cost\n",
        "    total_cost = uncached_input_cost + cached_input_cost + output_cost\n",
        "\n",
        "    # Create a DataFrame with the detailed cost breakdown\n",
        "    cost_df = pd.DataFrame({\n",
        "        'Cost Type': [\n",
        "            'Uncached Input Tokens Cost',\n",
        "            'Cached Input Tokens Cost',\n",
        "            'Output Tokens Cost',\n",
        "            'Total Cost'\n",
        "        ],\n",
        "        'Tokens': [\n",
        "            uncached_tokens,\n",
        "            cached_prompt_tokens,\n",
        "            completion_tokens,\n",
        "            total_tokens\n",
        "        ],\n",
        "        'Cost (USD)': [\n",
        "            uncached_input_cost,\n",
        "            cached_input_cost,\n",
        "            output_cost,\n",
        "            total_cost\n",
        "        ]\n",
        "    })\n",
        "\n",
        "    # Format the Cost column to show 4 decimal places\n",
        "    cost_df['Cost (USD)'] = cost_df['Cost (USD)'].map('${:,.4f}'.format)\n",
        "\n",
        "    display(cost_df)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "2zpMAU4T3yAh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}